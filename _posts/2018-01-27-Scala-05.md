---
layout: post
title: Scala - JDBC & Log
category: scala
---

[参考](http://www.runoob.com/scala/scala-tutorial.html)

# 时间
```scala
import java.text.SimpleDateFormat  
import java.util.Date 
 
val dateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
val currenttime = () => dateFormat.format(new Date())
 
currenttime() // 时间字符串
 
```
 
# 数据库
```scala
import java.sql.DriverManager
import java.sql.Connection
import scala.io.Source
 
val driver = "com.mysql.jdbc.Driver"
val url = "jdbc:mysql://localhost:3306/chen"
val username = "root"
val password = "root" 
var conn:Connection = null
 
Class.forName(driver)
conn = DriverManager.getConnection(url, username, password)
 
// 如果是当个的查询，或者更新，不需要设置手动提交，直接执行query即可
// 批量插入：
conn.setAutoCommit(false); // 设置手动提交 
val sql2 = "insert into filter_rule (ifid,threshold) values (?, ?)";
val psts = conn.prepareStatement(sql2);
val files = Source.fromFile(INPUT_FILE).getLines
while(files.hasNext){
    val line = files.next();
    val kv = line.split(',');
    val ifid = kv(0)
    val threshold = kv(1).toDouble
    psts.setString(1, "0.1");  // 指定第几列（对应的问号），内容，还要注意输入格式
    psts.setDouble(2, threshold); 
    psts.addBatch(); // 添加
}
psts.executeBatch()  // 批量插入
conn.commit()		 // 手动提交
// 单个查询
val statement = connection.createStatement()
val resultSet = statement.executeQuery("select name, password from scala_t")
while ( resultSet.next() ) {
    val name = resultSet.getString("name")
    val password = resultSet.getString("password")
    println("name, password = " + password + ", " + password)
}
 
```
 
 
# 日志

针对Spark应用的日志，使用Spark自己的那套，extends Logging，用的时候直接用 
如 log.info("Starting Streaming Processor.")，
在spark-submit时加上参数
./spark-submit \
      --conf "spark.executor.extraJavaOptions=-Dlog4j.configuration=file:/opt/workspaces/apps/stream-procoessor/log4j-executor.properties"，
 
log4j-executor.properties是自定义的日志文件：
```java
log4j.rootCategory=INFO, console, file
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

log4j.appender.file=org.apache.log4j.DailyRollingFileAppender
log4j.appender.file.file=/opt/workspaces/apps/stream-procoessor/logs/log.log
log4j.appender.file.DatePattern='.'yyyy-MM-dd
log4j.appender.file.layout=org.apache.log4j.PatternLayout
log4j.appender.file.layout.conversionPattern=[%-5p] [%d{yyyy-MM-dd HH:mm:ss}] [%C{3}:%M:%L] %m%n
log4j.appender.file.encoding=UTF-8

log4j.logger.com.asto.dop=DEBUG
```
关键的两点是**root、INFO**级别
项目包（这里是com.asto.dop）级别用DEBUG这样可避免过多 系统DEBUG的干扰
需要注意的是这里指定了DailyRollingFile路径，这会短路它自己的日志文件（work下的那个）即web ui上不会输入日志。
 
针对普通scala应用的日志，使用scala-logging-slf4j+logback，使用上很简单extends LazyLogging，然后也直接用，如logger.trace(s"Thread [${Thread.currentThread().getId}] produce log : ${message.toString}")

如果不想自定义日志文件又想改日志级别可以用编程的方式，在启动时加上Logger.getRootLogger.setLevel(Level.TRACE)，但这个编程的方式好像只能改全局日志级别，如果是DEBUG的话，框架打印出来的日志很巨量，你的业务代码日志完全被淹没了…… 
 
```scala
import org.slf4j.LoggerFactory  
import com.typesafe.scalalogging.slf4j.Logger  
  
  
object LogTest extends App {  
  val logger = Logger(LoggerFactory.getLogger("name"))  
    
  logger.debug("This is very convenient ;-)")  
}  


```