---
layout: post
title: Spark - Install Standalone
category: spark
---

# 准备工作

+ Java环境的安装，同时将JAVA_HOME、CLASSPATH等环境变量放到主目录的.bashrc，执行source .bashrc使之生效
+ 选择合适的版本下载，可以考虑是否自行编译
+ Spark需要Hadoop的HDFS作为持久化层，所以在安装Spark之前需要安装Hadoop，这里Hadoop的安装就不介绍了
+ （可选）实现创建hadoop用户，Hadoop、Spark等程序都在该用户下进行安装
+ ssh无密码登录，Spark集群中各节点的通信需要通过ssh协议进行，这需要事先进行配置。通过在hadoop用户的.ssh目录下将其他用户的id_rsa.pub公钥文件内容拷贝的本机的authorized_keys文件中，即可事先无登录通信的功能

# 修改配置

如果是编译好的版本，直接解压，修改conf文件夹下的配置文件，比如：
spark-env.sh
```bash
export SPARK_MASTER_IP=[master ip]
export SPARK_MASTER_PORT=7077
```
slave：添加从节点host或者IP
core-site.xml：修改HDFS相关配置
```xml
<property>
    <name>hadoop.job.ugi</name>
    <value>用户名,密码</value>
    <!-- 需要替换 -->
</property>
<property>
    <name>fs.default.name</name>
    <value>hdfs://host.ip:54310</value>
    <!-- 有权限的hdfs服务地址 -->
</property>
```

## 配置文件说明
Spark 2.x 系列中，针对配置文件做了一下修改，不在使用`hadoop-site.xml`文件，采用 `hdfs-site.xml`，`mapred-site.xml`代替


|名称|作用|备注|
|:---- |:----|:----|
|*Spark相关*|
|core-site.xml|Spark主要的配置，包括读写hadoop相关的配置，安全配置，权限和其他控制等|全局作用域|
|spark-defaults.conf|Spark默认配置，包括运行方式、端口、日志和包等信息|优先级最低，代码写死 > 提交时的额外配置 > default|
|spark-env.sh|Spark任务启动前执行，添加各种环境变量，文件中有详细的说明|可以不配置|
|*HDFS相关*|
|hdfs-site.xml|在Spark的配置中，只需要添加一些读写hdfs的配置|在hadoop或者hdfs client中需要详细|
|mapred-site.xml|填写job tracker地址||
|hive-site.xml|hive相关配置|如果没有使用则不用配置|
|*其他*|
|yarn-site.xml|集群节点服务器地址||
|log4j.properties|控制本地或者集群日志的输出情况|控制集群日志时，需要通过 --files 提交该文件|
|*HDFS/Hadoop 配置*|
|hadoop-site.xml|等于上边core-site.xml中hadoop部分和HDFS相关配置的组合|需要能成功执行 hadoop fs -ls 或hdfs dfs -ls|


+ 最后一个文件是hadoop或者hdfs的配置文件`$HADOOP_HOME/conf/`，前面的都是spark的配置文件,`$SPARK_HOME/conf/`
+ `hadoop.job.ugi` 用来配置hadoop需要的用户名密码，必须要正确，且和hadoop的各个服务器匹配
+ `fs.default.name` 用来配置默认的hdfs文件开头，在代码中 /config/conf.xml 会被解析成 hdfs://default_host:port/config/conf.xml，因此对于 --files 之外的路径，均需要真实存在与hdfs之中，且有权限。
+ spark-defaults.conf 中可以修改端口，避免被占用或者被屏蔽，比如：spark.ui.port=8888

# 程序部署
Spark程序直接复制到其他几台机器上即可
然后执行`sh $HADOOP_HOME/sbin/start-all.sh`启动即可

需要注意的是，每个节点的HDFS Client都要正常才可以

# 任务提交

```bash
./bin/spark-submit \
    --class org.apache.spark.examples.SparkPi\
    --master yarn \
    --num-executors 3 \
    --driver-memory 4g \
    --executor-memory 2g \
    --executor-cores 1 \
    --driver-class-path jars/spark-yarn_2.11-2.1.0.2.jar \
examples/jars/spark-examples*.jar 10
```

可以在 sprak_host:8080 查看集群和任务状态

# 遇到的问题
+ 版本问题：运行Spark相关程序提示 shell 语法错误
> 新版Spark本要求 bash 版本大于3.1，如果是centos 4.3的版本很可能就是低版本的bash，可以上官网下载(http://www.gnu.org/software/bash/)升级。
> 另外，对于java 1.7+ 和 scala 也有版本要求，尽量用新的。

+ YARN 平台：Yarn application has already ended! It might have been killed or unable to launch application master...
> 通常这是Yarn报错都有的一句话，只从这句话看不出什么毛病，如果日志中【INFO等级】只有这个信息，没有其他可疑项目，那就是yarn-site.xml配置有问题

+ jar 问题：Could not find or load main class org.apache.spark.deploy.yarn.ExecutorLauncher 或者 ApplicationMaster
> 由于某些jar没有传到executor或者driver导致的，通过提交任务时添加 --driver-class-path   jars/spark-yarn_2.11-2.1.0.2.jar 解决，或者用 --jars

+ HDFS 权限：org.apache.hadoop.security.AccessControlException: Permission denied
> 这个错误底下通常会告诉你用什么用户访问导致的没有权限，可能是core-site.xml或hdfs-site.xml配置正确。
> 也有可能是在程序中有不正确的打开路径，比如open了一些没有的文件，那程序会去hadoop里开，hadoop里找不到，就是这个错误。另外给Spark 设置 checkpoint 也要注意地址
> 添加外部文件通过 --files 标签来提交，然后在程序中可以按照打开本地文件的方法来读取。

+ 文件编码：java.nio.charset.MalformedInputException: Input length = 1 ...
> 读取的外部文件是其他编码的比如GBK，暂时的处理方法，去掉中文。。

+ spark-submit提交到集群后，找不到target文件
> 可以手动复制到worker对应的路径上

+ 无法连接Hadoop，出现spark error received signal term
> 更新jar：libdfs-java-2.0.2.jar -> libdfs-java-2.0.5.jar

+ standalone模式无法写Hadoop，出现timeout。local模式可以
> 某些worker节点没有安装HDFS client 