---
layout: post
title: ML - Model Evaluation and Selection
category: ml
---

> 周志华《机器学习》 第二章 模型评估与选择

# 评估方法
### 留出法(hold-out)
* 将数据集随机划分为两个互斥的训练集和测试集，要求两个数据集分布相同避免产生而外的偏差

通常进行多次随机划分和测试，采用所有结果的均值

### 交叉验证法(cross validation)
又叫k折交叉验证(k-fold cross validation)
* 将数据集划分为k个大小相近的互斥数据集，每个子集与总体分布接近

测试时每次选择其中一个作为测试集，剩余的作为训练集，需要进行k次测试。通常要进行多轮k折交叉验证取均值
> k的数值等于数据集的样本数时，为【留一法】，能比较真实的反应数据集情况，但是成本比较高

### 自助法(bootstrapping)
* 从数据集中有放回的采样，得到和原来数据集大小相同的训练集，没有出现在训练集中的样本作为测试集，假设数据集中有m个样本，对(1 - 1/m)^m取无穷大极限，则剩余样本的比例约为 1/e

优势是数据集规模相同，但是分布会出现变化，引入估计偏差。适合在数据集小的情况下使用，适用于集成学习等方法。

# 性能度量(performance measure)
回归任务多采用均方误差(mean squared error)进行性能度量，以下讨论的主要是分类任务

### 错误率(error rate)与精度(accuracy)
错误率：分类错误样本数 / 样本总数
精度：分类正确样本数 / 样本总数 = 1 - 错误率
对于连续的情况，则改为积分

### 准确率(precision)与召回率(recall)

|真实|预测（正）|预测（反）|
|---|---|---|
|正例|true positive|false negetive|
|反例|false positive|true negative|
 

* 准确率：P = TP / (TP + FP) 
> 判断为A的结果中，真正为A的比例

* 召回率：R = TP / (TP + FN)
> 判别为A的结果，占全部A的比例

将结果按照可能性进行排序，在逐个把结果作为正例进行预测，可以得到『P-R曲线』用来衡量性能

* F1 = 2*P*R / P+R
> harmonic mean: 1/F1 = 1/2 * (1/P + 1/R) 

### ROC与AUC
多应用于对于预测值 + 阈值进行比较做出分类的算法
#### 受试者工作特征 ROC(Receiver Operating Characteristic)
对于预测结果，从最可能到最不可能进行排序，将阈值两边的结果划分为正例和反例，然后调整阈值得到ROC曲线
* 纵轴：真正例率 TPR(True Positive Rate) = TP / (TP + FN)
> 判断为正例占全部正例的比例，与召回率相同

* 横轴：假正例率 FPR(False Positive Rate) = FP / (TN + FP)
> 错误判断为正例，占全部反例的比例

特殊点：
> 阈值设置为最大（最严格），此时没有正例，对应图中原点
> 阈值设置为最小（最不严格），此时全部都是正例，对应图中(1,1)

过程：（样本中，正例为m1个， 反例为m2个）
从原点处依次将阈值设置为每一个样例的预测值，即依次将每个样例划分为正例
设前一个标记点的坐标为(x,y)，考察当前样例真实情况
如果为真正例，则当前坐标为(x, y + 1/(m1))
如果为假正例，则当前坐标为(x + 1/(m2), y)

ROC曲线的面积即为AUC(Area Under ROC Curve)，AUC可以反应样本的排序质量

### 代价敏感错误率与代价曲线
为了使得算法能避免某些特定的错误，或者提高某个类别的正确率，设置代价矩阵，使得错误率的计算受到类型的影响
此时ROC曲线不能直接反应出期望总体代价，使用代价曲线(cost curve)

过程：
对于ROC上每一点(FPR, TPR)，算出对应的FNR
然后绘制一条从(0, FPR)到(1, FNR)的直线，线段下的面积表示该条件下的期望总体代价
将所有点都转化为线段，所有线段之下的面积，即为所有条件下学习期的期望总体代价

# 比较检验
在评估方法和性能度量的基础上，有效合理的比较性能度量的结果
### 假设检验



# 偏差与方差

