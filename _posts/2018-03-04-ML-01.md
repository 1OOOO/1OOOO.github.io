---
layout: post
title: ML - iForest (Isolation Forest)
category: ml
---
# iForest(Isolation Forest) 孤立森林异常检测

## 原理

[参考：Ye Zhu](https://www.jianshu.com/p/5af3c66e0410?utm_campaign=maleskine)

iForest （Isolation Forest）孤立森林 是一个基于Ensemble的快速异常检测方法，具有线性时间复杂度和高精准度，是符合大数据处理要求的state-of-the-art算法[参考：Outlier Analysis](http://charuaggarwal.net/outlierbook.pdf)。其可以用于网络安全中的攻击检测，金融交易欺诈检测，疾病侦测，和噪声数据过滤等。本文将通俗解释实现方法和日常运用，即无需深厚的数学功底。

首先，我们先了解下该算法的动机。目前学术界对异常（anomaly detection）的定义有很多种，iForest 适用与连续数据（Continuous numerical data）的异常检测，将异常定义为“容易被孤立的离群点 (more likely to be separated)”——可以理解为分布稀疏且离密度高的群体较远的点。用统计学来解释，在数据空间里面，分布稀疏的区域表示数据发生在此区域的概率很低，因而可以认为落在这些区域里的数据是异常的。

不同算法对异常的判断[结果示例](http://scikit-learn.org/stable/auto_examples/covariance/plot_outlier_detection.html#sphx-glr-auto-examples-covariance-plot-outlier-detection-py)：
![Outlier detection with several methods](http://scikit-learn.org/stable/_images/sphx_glr_plot_outlier_detection_001.png)
![Outlier detection with several methods](http://scikit-learn.org/stable/_images/sphx_glr_plot_outlier_detection_003.png)

iForest属于Non-parametric和unsupervised的方法，即不用定义数学模型也不需要有标记的训练。对于如何查找哪些点是否容易被孤立（isolated），iForest使用了一套非常高效的策略。假设我们用一个随机超平面来切割（split）数据空间（data space）, 切一次可以生成两个子空间（想象拿刀切蛋糕一分为二）。之后我们再继续用一个随机超平面来切割每个子空间，循环下去，直到每子空间里面只有一个数据点为止。直观上来讲，我们可以发现那些密度很高的簇是可以被切很多次才会停止切割，但是那些密度很低的点很容易很早的就停到一个子空间了。上图里面黑色的点就很容易被切几次就停到一个子空间，而白色点聚集的地方可以切很多次才停止。

### 步骤

1. 从训练数据中随机选择n个点样本点作为subsample，放入树的根节点。
2. 随机指定一个维度（attribute），在当前节点数据中随机产生一个切割点p（切割点产生于当前节点数据中指定维度的最大值和最小值之间）
3. 以此切割点生成了一个超平面，然后将当前节点数据空间划分为2个子空间：把指定维度里小于p的数据放在当前节点的左孩子，把大于等于p的数据放在当前节点的右孩子。
4. 在孩子节点中递归步骤2和3，不断构造新的孩子节点，直到 孩子节点中只有一个数据（无法再继续切割） 或 孩子节点已到达限定高度 。

获得t个iTree之后，iForest 训练就结束，然后我们可以用生成的iForest来评估测试数据了。对于一个训练数据x，我们令其遍历每一棵iTree，然后计算x最终落在每个树第几层（x在树的高度）。然后我们可以得出x在每棵树的高度平均值，即 the average path length over t iTrees。* 值得注意的是，如果x落在一个节点中含多个训练数据，可以使用一个公式来修正x的高度计算，详细公式推导见原论文。

### 补充

1. iForest具有线性时间复杂度。因为是ensemble的方法，所以可以用在含有海量数据的数据集上面。通常树的数量越多，算法越稳定。由于每棵树都是互相独立生成的，因此可以部署在大规模分布式系统上来加速运算。
2. iForest不适用于特别高维的数据。由于每次切数据空间都是随机选取一个维度，建完树后仍然有大量的维度信息没有被使用，导致算法可靠性降低。高维空间还可能存在大量噪音维度或无关维度（irrelevant attributes），影响树的构建。对这类数据，建议使用子空间异常检测（Subspace Anomaly Detection）技术。此外，切割平面默认是axis-parallel的，也可以随机生成各种角度的切割平面，详见“On Detecting Clustered Anomalies Using SCiForest”。
3. iForest仅对Global Anomaly 敏感，即全局稀疏点敏感，不擅长处理局部的相对稀疏点 （Local Anomaly）。目前已有改进方法发表于PAKDD，详见“Improving iForest with Relative Mass”。
4. iForest推动了重心估计（Mass Estimation）理论发展，目前在分类聚类和异常检测中都取得显著效果，发表于各大顶级数据挖掘会议和期刊（如SIGKDD，ICDM，ECML）。

### 论文

[Isolation Forest](http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf)
[Isolation-based Anomaly Detection](http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tkdd11.pdf)

## sklearn 使用

[sklearn.ensemble.IsolationForest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)

```python
class sklearn.ensemble.IsolationForest(
    n_estimators=100,   # 判决树的数量
    max_samples=’auto’, # 每个树的样本数
    contamination=0.1,  # 异常比例
    max_features=1.0,   # 每个树的维度数
    bootstrap=False,    # 一棵树选择样本时是否进行由放回的采样
    n_jobs=1,           # 任务并行数
    random_state=None,  # 随机数生成方法，默认np.random
    verbose=0           # 是否输出训练日志
)
```
